# 3. Interpetive Feedback Effects and Public Opinion on the Affordable Care Act: A Survey Experiment

- Abstract
- Be more specific about alternative specifications in footnotes (if necessary? check this on next editing pass).

## Introduction

While the Patient Protection and Affordable Care Act (ACA) was passed over six years ago, the political debate over the health reform law has not yet ended. There have been numerous court cases about the law and specific parts thereof, with the court declaring a key piece of the law, the Medicaid expansion requirement, unconstitutional and leaving the decision up to the states. Many states have refused to expand Medicaid, and have also opted to use the Federal insurance exchange rather than setting up their own state exchanges as the law's architects initially envisioned. The House of Representatives has voted dozens of times to repeal the ACA. One recent attempt actually made it through the Senate (on reconciliation, which waives the Senate's supermajority requirements for cloture) and to the President's desk, where it was promptly vetoed. The fact that a repeal bill made it that far for the first time six years after passage and well into implementation is striking, and signals that at least some Republicans really are serious about repealing the ACA and do not feel much pressure from beneficiaries of the law to back down. If Republicans hold Congress and take the White House in the 2016 elections, the last obstacle to those efforts (a Democratic president's veto) will effectively have been removed. All three remaining Republican candidates have vowed to repeal Obamacare. In their analysis of the ACA's shaky political foundation, Oberlander and Weaver conclude "the fight over Obamacare is not over" [@Oberlander2015].

Public opinion on the ACA is still mixed, with small but persistent pluralities in opposition [@DiJulio2015; see also @Jacobs2016]. This matters for the law's future. Public opinion affects health policy through elections and other influences on elected officials [@Jacobs2000; @Jacobs1993; @Skocpol1994; @Skocpol1996], who will ultimately decide the law's fate. For example, the refusal of some states to expand Medicaid and even to close down otherwise successful state health insurance exchanges (as Kentucky is in the process of doing) are a result of continued public opposition and elected officials who represent that opposition in ways that hinder the ACA in achieving its goals. To make predicitons about the political future of health reform in America, we need to understand public opinion and how it might change (or not).

One of the most important factors to examine is the effect of the law itself, as it is both implemented and interpreted, on public attitudes. Was the law written in such a way that it will soon become entrenched and impossible to remove, like Social Security and Medicare, or will it fail to cultivate a large enough support coalition and collapse, be repealed, or be watered down to the point of insignificance? Policy feedback theory, which I will discuss in detail below, provides a few key insights. In this study, I present findings from a survey experiment designed to examine how the ACA's provisions might be interpreted by the public and thereby affect public opinion, filling a gap in the feedback literature on interpretive effects of policy design on mass attitudes. Filling this gap in turn allows me to use policy feedback theory to comment on the strategies and outcomes that can be expected with respect to public opinion on health reform.

Briefly, the results suggest that both opinions of the law and the way those opinions are formed can change depending on what information is in the person's mind at the time. Based on past research and the findings of this study, I argue that the ACA is written in such a way that different elements of the law can invoke different public responses to the law as a whole. Because material benefits of the law will, by design, remain somewhat narrowly focused, I argue that interpretive feedback effects will be more relevant to the health reform debate than material/ resource effects. To the extent this argument holds, there are strong reasons to suspect that opinion on the ACA will remain fundamentally divided, ensuring neither side of the debate will back down any time soon. The application to policy feedbacks also allows my findings to be applied qualitatively to other policies and their effects on public opinion.

## Background

Previous research has advanced our understanding of public opinion on health reform in a few key ways. Partisanship is highly correlated with peoples' opinions of the law [@DiJulio2015]. Polling has also shown that although slim majorities have generally opposed the ACA as a whole, voters overwhelmingly favor some parts of the law [@Brodie2010]. However, there is a sizable portion of voters who do not express any opinion on the ACA [@DiJulio2015], and research shows these people are systematically different from those who do express opinions in terms of demographic and socioeconomic characteristics [@Berinsky2011]. A racial element of opinion on health reform has also arisen, casting the reform in implicitly racial terms [@Henderson2011; @Tesler2012]. These findings suggest some general contours for opinion on health reform, but our knowledge about how the landscape might shift is less certain. Understanding the dynamics of public opinion is important in predicting how the debate will play out.

There have been a few public opinion studies on these dynamics, showing the potential for changes in ACA approval based on certain factors. Support for the ACA or health reform in general is correlated with favorability toward specific provisions in the law [@Grande2011], increased knowledge of the law [@Gross2012], partisanship and self-interest [@Gross2012; @Henderson2011], and beliefs about who deserves government assistance [@Gollust2011]. Using a panel survey design, @Jacobs2016 analyzed within-person trends in ACA opinion over time, as well as the effects of partisanship, perception of direct benefits from health reform, and other factors on those overall opinions. They find that opinion on the law has remained divided, with personal opinions remaining largely entrenched over time, but with significant decreases in support for repealing the ACA. While they conclude that outright repeal of the ACA grows less likely with the passage of time, there is also no evidence that political support for the law will increase any time soon in the manner that reformers initially hoped.

As @Jacobs2011 point out, it is not enough to understand the short-term effects of "situational framing" on ACA opinion. If we want to know what will happen to the law politically in the long term, we need to examine "structural framing." One of the most important sources of structural framing is the design of the ACA itself and the way that design is used to structure public discourse on the law.

Policy feedback theory has already proven to be a useful analytical tool in examining this aspect of the ACA. Much has been written about policy feedbacks and their specific application to health policy and the ACA [for example, @Oberlander2015; @Campbell2011]. In general, the theory predicts that politicians trying to get a major policy passed will attempt, wherever possible, to package the proposal so that concentrated benefits are realized sooner and costs incurred later [@Pierson1993; @Pierson2000; @Campbell2012; @Oberlander2015]. This serves to build a coalition of political support early in a program's existence. There are constraints on would-be reformers' ability to accomplish this, such as budgetary and administrative considerations, political coalitions of elected officials, and existing stakeholder interests [@Oberlander2015; @Patashnik2013].

One of the main assumptions of policy feedback theories relevant to this discussion is that policy design affects opinions [@Campbell2011, 962]. This is expected to occur either through interpretive effects, or the ways in which the public perceives or interprets a policy, or resource-based effects, whereby specific groups receive direct tangible benefits and thereby have a vested interest in preserving the program that confers those benefits [see @Pierson1993]. Interpretive effects can be examined at the level of the general public or program beneficiaries specifically [@Campbell2012]. Interpretive effects are important to consider because the other feedback mechanism through which mass opinion might be influenced, resource effects, can only affect part of the population in the near-term,[^1] whereas interpretive effects are manifest among the public as a whole. Part of the design of the ACA was to expand insurance coverage as much as possible while leaving the rest of the system untouched [@Oberlander2015]. This is one of the key ways in which constraints on design (opposition to disrupting the status quo, in this case) affect policy design. Although some policies may be designed to build resource-based support among large portions of the population and may be successful at doing so, the ACA was not and could not be designed in this manner. Unless the ACA remains intact long enough to deliver *recognizable* benefits to a substantial majority of the population, the balance of overall opinion on health reform will be based mainly on political interpretations. Therefore, this study focuses on the interpretive effects assumed to exist in policy feedback theory.

At a system level, this prediction requires some link between opinion and policy making. In other words, aggregate responsiveness of policy to public opinion and vice versa must occur. Various findings of a thermostatic link between opinion and policy [@Erikson2002; @Soroka2010] provides one such plausible system-level mechanism. As policy changes, the basic model predicts, an attentive public will respond by changing its expressed desire for policy change. Thus, it is plausible to assume, as policy feedback studies do, that policy and aggregate public opinion can feed back onto each other.

What these macro-level models lack, for purposes of validating this hypothesized link, is an individual-level mechanism and evidence that individuals actually do interpret differently-designed policies differently. Without such a plausible causal link, the individual-level levers posited by feedback scholars may not be leading to observed aggregate responses. The memory-based model of the survey response [@Zaller1992a] provides a plausible mechanism. According to this model, a person asked to respond to a survey question first retrieves a few relevant pieces of information. This is not an exhaustive (or exhausting) search of one's entire memory, but rather a nonrandom sample of considerations conveniently at the "top of the head." The retrieved considerations are processed and mapped onto possible answers to the question. Thus, reported attitudes may change significantly, depending on what relevant information has been on a respondent's mind recently [@Zaller1992a]. To the extent that policy design elements can serve as considerations in opinion formation, this mechanism is consistent with the predictions of policy feedback theory.

The existence of these mechanisms supports policy feedback predictions of attitude change in important ways. However, there is still one crucial link in the chain left to test: do elements of policy design affect the opinions people form about the policy? The idea that policies will change opinions cannot be taken for granted, nor is this tested specifically by the memory-based model or the macro-level models. The macro models merely show that opinion change follows policy change, which is not sufficient to say that this public response would be altered if policies were designed or interpreted differently. The memory-based model, as mentioned, provides a plausible individual-level mechanism for the required attitude change, but not a guarantee that the mechanism will be used or that overall opinion change will occur. Despite the strength of the policy feedback theory's predictions of attitude change, policy feedback scholars must still design their own tests to verify that the mechanisms described above are actually manipulated by policy.

Quantitative studies in the policy feedback tradition find mixed evidence on this point [@Morgan2011; @Soss2007; @Campbell2012]. The evidence is especially thin with regard to effects on opinion generally, as opposed to opinion among program beneficiaries. To my knowledge, there have been no causal studies of the effect policies, or at least public understanding of policies, have on attitudes. Thus, there is a significant gap in the literature on policy feedbacks surrounding this key prediction of attitude change, limiting the analytical utility of applying policy feedbacks to public opinion on the ACA. Closing this gap would allow us to comment more specifically on the ACA's future with public opinion.

The present study fills this gap using a survey experiment to examine whether priming different aspects of a reform package changes overall opinions. This procedure mirrors survey experiments reported in the survey methodology literature, such as a classic experiment reported by @Schuman1981 in which respondents were much more likely to favor allowing communist reporters into the US when they were first asked whether the Soviet Union should admit American reporters. This and other similarly designed experiments have been shown to change the "weights respondents give to the factors relevant to answering a question" [@Krosnick2010, 293]. Furthermore, asking specific questions about a topic before asking more general questions can affect the context in which answers to the more general questions are generated [@Krosnick2010]. This use of priming thus operates according to the mechanism predicted by the memory-based model. To put it in the context of this study, asking specific questions about ACA provisions may cause some respondents to report opinions about the ACA in general that are more correlated with their answers to the specific questions than they would be in the absence of the prime. These are exactly the kinds of effects we would expect to see given policy feedback theory's predictions, which I will operationalize in the next paragraphs.

At least two different types of design features may be included in a proposed bill. I will call the first a "slam dunk" proposal. This is an almost universally favored provision of a law that can feature prominently into political messaging. It may be designed more for political gain than to actually advance the policy aims of the overall proposal, though it may do both. One such slam dunk policy in the ACA is the requirement that young adults be alloowed to remain on their parents' insurance plans until age 26, which I will refer to as the dependent coverage provision for short. This provision is almost universally favored [@Brodie2010]. ACA supporters could quickly go on offense with slam dunks like these, promising even more good things to come if people will just stick with this newfangled health law (while downplaying the costs that must also come eventually). These design features can increase political support very effectively.

Slam dunks must be executed effectively; they are not to be taken for granted. These provisions may be entirely severable from the rest of the program, meaning they have to be tied rhetorically to the other parts of the law. It is not obvious that allowing dependents to remain on their parents' insurance until age 26 goes along with a requirement for most individuals to buy health insurance.[^2] The severability of the slam dunk provision from the rest of the law means that although it will probably stand on its own, it may not be potent enough to shield other parts of the ACA from attacks in the face of sustained opposition. If the slam dunk does not break down opposition to a new program quickly, before the more controversial parts are implemented, then it may not be of much political use in those later stages.

The other type of design feature I will call a potential wedge issue. I say "potential" because some outside effort is required to make these provisions politically potent. If those in favor of the policy control the discussion, the wedges will not be hammered down politically (though they might be felt in other ways). The individual mandate, the ACA's requirement that most individuals purchase health insurance or pay a fine, is an example of a potential wedge issue. This controversial provision has varying levels of support, depending on the way it was described in survey questions [@Grande2011], giving it the potential to enhance public divisions on the whole reform package. Politicians mindful of feedback effects try to include as few wedge issues as possible in a new law in order to increase its chances of political success. They want to consolidate support, not divide it, and they want to keep political opponents from creating such divisions. However, depending on the constraints imposed on would be reformers, some wedge issues are unavoidable. The decision to try to leave as much of the current status quo intact as possible meant that the ACA almost certainly had to include an individual mandate. From the reformers' point of view, the political strategy was to build initial support using some slam dunks so that as the potential wedge issues were implemented, their divisive effects would be muted. Opponents, given that they were unable to stop the policy from passing, hoped to use these wedge issues to prevent such a support coalition from fully forming. However, if their base of support crumbled early as a result of some effective slam dunks from the other side, the wedge would lose its potency.

Based on policy feedback theory and the concepts described above, we can derive two hypotheses to be tested in this study. First, we might expect that if provisions within the ACA are primed before overall opinion of the ACA is reported, then those overall opinions will be different than if those provisions were not salient. Put another way, if respondents are primed to think about a specific provision, their average reported favorability of the ACA would differ from the favorability reported by otherwise similar respondents not primed to think about the provision. Different provisions may have different effects. If a slam dunk provision is primed, we would expect to see movement in favor of the ACA. Wedge issues, on the other hand, may not move aggregate opinion when primed, but they may move the opinions of certain subgroups in opposite directions and shore up opposition to the ACA overall. We would expect different provisions to move opinion differently, such that priming provisions that tend to be viewed more favorably will move ACA opinion in a positive direction.

Second, given the plausible individual mechanism presented in the memory-based model, we might expect the way in which responses to the ACA favorability question are formed to vary according to what information is salient at the time of response. If a specific provision is salient (because of priming) in a respondent's mind at the time ACA favorability is reported, then the respondent's opinion of the ACA will be more strongly affected by his/ her opinion of the salient provision. Conversely, if no specific provisions are primed, then other factors will be more important in forming the opinion. 

By testing these hypotheses, we can assess whether certain design aspects of the law affect the public's interpretation of and favorability toward the ACA as a whole. Note this is not a test of favorability towards the provisions themselves, nor is it an estimate of the population parameter for ACA favorability. The hypotheses deal with changes in average opinion, not estimates of average opinion itself. Note also that this is not a test of the memory-based model, as the design will be between-person rather than within-person. Data in support of these hypotheses would, however, increase confidence that priming policy provisions (thereby making respondents interpret the law differently) causes aggregate opinion change.

## The Study

To test the hypotheses raised above, I conducted a survey experiment [@Mutz2011] in November 2014 on Amazon's Mechanical Turk (MTurk) service. MTurk respondents have the opportunity to complete a wide range of simple tasks, such as tagging images, coding expense reports, and taking surveys, in return for small financial rewards. Respondents opt in to each task, using MTurk's user interface that presents a list of available tasks and can be filtered or sorted by task metadata such as keywords, date, and payment amount. The exact topic of my survey was concealed until after respondents had opted in. Respondents knew they would take a survey, but not that it would deal specifically with the ACA. Implications of the choice of this subject pool are discussed later.

In the experimental portion of the survey, random assignment was used to sort respondents into one of four experimental conditions, outlined in Figure 1. In two of the experimental groups, respondents were first asked to respond to two questions about their opinion of the ACA in general. These two groups then responded to questions about specific pieces of the law. One group answered questions about the individual mandate (the requirement that most individuals purchase health insurance or pay a fine), while the other was asked about the provision requiring that adults under age 26 be allowed to remain on their parents' insurance plans. I refer to both of these groups as the unprimed or control groups. For the other two groups, which I refer to as the primed or treatment groups, the order of the two question blocks was reversed, so the provision-specific questions came first and the general ACA questions appeared afterward. The question wordings and their order within the blocks remained constant. Only the order of the blocks and the specific provision referred to in the questions was changed.

![Figure 1: Survey Experiment Design](C:\Users\Robbie\Documents\dissertation\Analysis\Figs\fig3-1.png)

By manipulating the order of the questions, respondents in different groups had different considerations "at the tops of their heads" when responding to the general ACA questions. The primed groups had been primed to think about the specific provisions they were asked about immediately prior to giving their opinions of the law as a whole.

In the initial wave of the experiment (Nov. 22 and 24, 2014), 506 responses were collected. Each respondent was paid \$0.40 for completing the survey (repeat responses were not allowed). This wave included my survey experiment and two separate vignette experiments conducted by two other researchers about racial group identity and terrorist recruiting messages. More information about the other two studies is available from the author upon request. My study appeared first, followed by the other two, with demographic questions at the end. After completing the experimental portion of the survey, respondents reported their level of approval of President Obama, their partisanship and political ideology, their racial identities and religious affiliation, their level of attentiveness to news and current events, their insurance status, and their income level. Specific question wording for my experimental questions and for demographic questions is given in Appendix C.

Upon preliminary examination of the data from the initial wave, it was discovered that the code for my portion of the survey had failed to randomize the order of the general ACA and specific policy questions. This meant I only had data for the two unprimed groups, my "control" groups. In order to collect data for the other two groups, I was able to put my survey experiment (along with the same demographic questions, but excluding the other two experiments) back into the field on MTurk on Nov. 25, 2014, with a compensation rate of \$0.12 per respondent.[^3] Those who had taken the survey previously were barred from participating again. This second wave yielded 396 additional responses. I randomized the treatment group assignments for this second wave in such a way that all four groups were represented, but with twice as many respondents assigned to the two groups that had been omitted in the first wave (the primed groups).

Collecting data on all four groups in the second wave allowed me to assess whether respondents to the two waves were different based on observed characteristics. I checked to ensure that there were no significant differences in the types of respondents to the two waves. Each wave had a different estimated completion time and a different level of compensation, in addition to being fielded on different days. However, there is no indication that responses were affected by these differences. Balance checks revealed that on all 16 items relevant to my survey (including both demographic and ACA-related items), only ideology differs significantly across waves, and that difference only appears for one specification of the variable. Specific results of these balance checks are reported in the tables in Appendix A, along with final sample sizes within each treatment group. The analyses in the main body of this paper combine data from both waves of the survey, with treatment presumed to be given at random. Results do not change significantly when respondents from the first wave are omitted.

### Sample Demographics and Descriptive Statistics

The use of MTurk as a survey platform has some limitations. As an opt-in survey conducted over the Internet, there is no way to randomly select respondents, nor is there any way to calculate response rates, refusal rates, margins of sampling error, or any of the usual survey metrics. The respondents to my survey do not resemble the demographic or political profiles of the US population, as can be expected from a subject pool such as MTurk. However, experimental results using MTurk have proven comparable to those obtained from other convenient subject pools often used in experiments [see @Mullinix2015]. Thus, for an experimental design, MTurk is an acceptable platform. For the hypotheses I am trying to test, internal validity is more important than external validity. I will report below how the respondents to my survey are different from the population, but I see no reason to expect that my results will differ qualitatively if applied to the general population. As it is not my intent to estimate population parameters, but rather to gauge the effectiveness of an experimental manipulation, data collected on MTurk is suitable for this study.

I asked respondents for their level of approval/ disapproval of President Obama, party identification (using the standard ANES question), ideology (liberal to conservative, 7-point scale with ANES wording but no branching), racial and religious identification, level of attentiveness to news, health insurance status, and income. As mentioned above, MTurk does not provide a good platform for representative survey results, and my sample is skewed on all observable characteristics in the ways we would expect of MTurk respondents. They are more liberal and Democratic, more likely to be uninsured, and have lower incomes than we would expect from a general US adult sample. Over half of the sample reported having no religious affiliation, and only about 34 percent identified as Christian. Table 1 gives the distributions of these political and demographic characteristics for the entire sample, as well as distributions of the opinion variables used in my study and population estimates (where available).

*Table 1: Descriptive Statistics*

**\[Table 1 about here\]**

Table Sources:

a. Population estimates obtained from Gallup’s Presidential Job Approval Center online tool [-@gallup]. The results for November 24-30, 2014, are reported.
b. Population estimates obtained from the 2014 GSS (variables partyid and polviews). For the party identification data, I combine independent leaners with independents for both surveys.
c. Population estimates obtained from the 2014 American Community Survey (single year data), accessed on American Factfinder.
d. Population estimates obtained from the 2010 wave of the Baylor Religion Survey [@Baylor2010]. The Baylor Survey included many more response options than the MTurk survey.
e. Population estimates obtained from Kaiser Family Foundation Health Tracking Poll, November 2014 [@kff2014]. For the insurance status question, 85% of KFF respondents said they had some form of insurance, but the survey did not ask for specific types of coverage. For the ACA opinion question, no middle response option was given on the KFF poll. For the ACA repeal question, I combined two of KFF’s response options ("scale back what the law does," 17 percent, and "expand what the law does," 22 percent) to obtain the middle category, "change some of it."

The table indicates very little missing data. Only five respondents skipped any questions, and these five account for all missingness in the data. The five incomplete responses are excluded only from models that include variables for which there is missing data.

Based on Table 1, we see that over half of all respondents favored the ACA. The US adult population reports considerably less favorable attitudes toward the ACA, as the population estimates in Table 1 show. We also see that most people knew that the specific provision they were asked about was in the final version of the law, and over half for each provision thought the provision would affect them or someone close to them. Attitudes toward the individual mandate were somewhat mixed, though as predicted more people opposed it than supported it. The dependent coverage provision had very high favorability in this sample, perhaps a result of the generally younger profile of MTurk users, though I do not have the data to support that hypothesis for this sample. The skewed distribution of opinion on the dependent coverage provisions may affect the results of some of my hypothesis tests, which is worth noting here as a potential limitation.

While having two other survey experiments in the study (between my experiment and the demographic questions) may have affected measures like party identification, ideology, and presidential approval, I do not find any systematic differences in these effects across groups. The randomization of the treatments was done separately for each experiment. Any effects of one experiment are statistically erased from the other groups by random assignment. There are no significant differences in the distributions of demographic variables across my experimental groups.

### Planned Analyses

In the results presented below, I set up comparisons based on the specific provisions about which each group was asked. For example, the unprimed group asked about the individual mandate (group 1 in Figure 1) is used as a control group, to be compared to the group first primed with questions about the individual mandate (group 2). Similar comparisons are made for the other two groups, with the primed group serving as a treatment group and the unprimed group serving as control.

To test hypothesis 1, I used t-tests of the difference in ACA favorability (a dichotomous variable, 1 = favor, 0 = oppose or neutral) across comparable treatment and control groups. This shows whether, as predicted, any differences in average opinion appear between primed and unprimed groups. I also break the four groups down by favorability toward the provisions asked about, which will prove useful in understanding what the data tells us about hypothesis 1.

Testing of hypothesis 2 employs ordered logit regression models of ACA favorability estimated separately for the two groups of policy conditions. Interactions between all variables and whether the respondent was primed are included to test for differences in specific coefficients across groups. Wherever an interaction term is significantly different from zero, we can conclude that the (conditional) correlation between that variable and ACA opinion is different across conditions. Additionally, Chi^2^-tests of the joint significance of the interaction terms tell us whether being in the primed group significantly changes the overall correlations. These tests will indicate whether the considerations used to form reported ACA opinion are different across primed and unprimed groups (within policy conditions).

## Results

If priming different considerations had any effect on opinions, as expected in the first hypothesis, we would expect to see differences in ACA approval levels between the treatment and control groups. Accordingly, I use basic t-tests to determine whether there were significant differences in the mean levels of ACA approval by experimental group. As displayed in Figure 2, the two unprimed groups hold similar distributions of opinions on the ACA, and changes occur in the expected directions for the primed groups. That is, support for the ACA is 16 percentage points higher for those primed to think about the popular dependent coverage provision compared to their unprimed counterparts (t = -3.22, p < 0.001), while priming the controversial individual mandate yields somewhat lower favorability, though this latter change is not statistically significant (t = 1.13, p = 0.261).

![Figure 2: ACA Favorability by Experimental Group](C:\Users\Robbie\Documents\dissertation\Analysis\Figs\fig3-2.png)

Note: Error bars represent 95% confidence intervals based on unpooled standard errors. The numbers underlying this figure appear in the supplementary online materials, Appendix B, Table B-1.

The slight decrease in favorability for the group primed with the individual mandate questions shown in Figure 2 is commensurate with the more mixed feelings toward the individual mandate expressed in the survey. The dependent coverage provision, by contrast, had much clearer positive effects on the other primed group's perceptions of the ACA. One possible reason for the less significant effect of the individual mandate is the nature of the prime. As @Grande2011 found in their study, feelings on the individual mandate were relatively more positive when using a description of the policy similar to the one I employed.

We might expect that priming the individual mandate will only negatively affect overall ACA opinions of those who oppose the individual mandate. Thus, another way to analyze this data is to break the groups down further into those who favored and did not favor each provision. Figure 3 shows this subgroup analysis (I group the middle response category with those opposed, hence the more inclusive label "do not favor"). Among those who do not favor the individual mandate, opinions of the ACA are 16 percentage points less favorable when primed (t = -2.71, p < 0.01). For the group that favored the individual mandate, the prime did not change opinions appreciably (t = -0.374, p = 0.709).

![Figure 3: ACA Favorability by Experimental Group and Provision Support](C:\Users\Robbie\Documents\dissertation\Analysis\Figs\fig3-3.png)

Note: Error bars represent 95% confidence intervals based on unpooled standard errors. The numbers underlying this figure appear in the supplementary online materials, Appendix B, Table B-1.

The same is not true of those in the dependent coverage conditions. No matter how individuals in this group felt about the specific provision, priming the provision increased favorability toward the ACA in general. In fact, those who did not favor the ACA saw increases in favorability of 28 percentage pointsas a result of the prime (t = 2.61, p < 0.05). This group is somewhat small (only 21 people in the primed portion had less than favorable opinions of this provision), limiting the precision of the estimates for this group. Still, we see evidence here that priming different pieces of the law had different effects on people's opinions. For the individual mandate, the effect depends on how the person feels about the individual mandate. For the dependent coverage provisions, the effect of the prime is not dependent on provision-specific opinions. The discussion section will elaborate further on why this qualitative distinction is important in the context of policy feedbacks.

The main findings in Figures 2 and 3 hold for various subsets of the sample and various specifications of the test. Using the continuous seven-point scale  as the dependent variable rather than the dichotomous variable yields qualitatively similar results. Effects are similar when the samples are split into those who knew the provisions made it into the final version of the law and those who did not, though it is interesting to note that the effects are stronger for those who did not know the dependent provision was in the law. The individual mandate prime was also stronger for those who did not know the individual mandate was in the ACA, but the prime was not significant for either group.

The results presented above might be biased if the question order affected respondents' opinions of the specific ACA provisions. Imagine an experiment designed to test the hypothesis that priming *general* ACA opinions affects opinions of a *specific* ACA provision, a causal ordering opposite my hypotheses. If we can use my experiment to support both causal stories, then we cannot logically conclude that either is valid because it means the provision-specific questions have been contaminated by order effects. However, in t-tests similar to the ones reported above, but with provision-specific opinions as the dependent variables, asking the overall ACA questions first creates no significant differences in opinions on either provision. For the individual mandate groups, t = -1.34 (p = 0.18), and for the dependent coverage groups, t = -0.67 (p = 0.51). For the dependent coverage groups in particular, this is remarkable given the potential for cognitive dissonance to decrease peoples' willingness to favor any part of a law they just said they opposed on the whole. That is what happened if we only look at the point estimates, but again that effect is not significant. Apparently, people are not as willing to judge the ACA's parts based on the whole as they are to judge the whole based on which parts are salient at the time of response.

We might also expect the null findings just described because of the way the experiment was set up. In the initial instructions, respondents are reminded that a "health reform bill known as the Affordable Care Act (ACA) or Obamacare was signed into law in 2010." The provision questions also referred back to the ACA, regardless of when they appeared in the survey. This means every respondent was "primed" to think about the ACA, with the only difference being that some of the respondents had answered two questions about the ACA before giving their opinions on specific provisions. Empirically, this difference in the level of the ACA prime does not seem to have had any effect, as the t-tests above show. Thus, it is reasonable to assume that the question order manipulation is not affecting opinions of the specific provisions. "Reverse priming" does not seem to be an issue for this study.

As predicted by our first hypothesis, ACA opinions appear to be subject to priming effects. Based on hypothesis 2, we also expect to see differences between treatment and control groups in terms of the predictors of ACA approval. To test this prediction, I estimated ordered logit regression models for the two policy groups separately, with the final models using level of ACA approval (a seven-point scale ranging from very unfavorable to very favorable) as the dependent variable and opinions of specific provisions, presidential approval, and partisanship as predictors. I have switched to the continuous variable for this analysis, as opposed to using the dichotmous variable, to take advantage of the greater variance in the continuous variable. As described above, an indicator variable for whether the person was primed or not and interactions between this indicator and all other predictor variables were also included. Other covariates like ideology, income, race, religion, and insurance status did not significantly affect the coefficients of interest when included, so I exclude those from the final models reported below.[^4] Because of the way the interaction terms are specified, the two models can actually be presented as four separate models, one for each experimental group, as in Figure 4, which plots the logit coefficients and 95% confidence intervals. Tables containing full model information, including the interaction terms, are found in Appendix B.

![Figure 4: Models of ACA Approval by Treatment Group](C:\Users\Robbie\Documents\dissertation\Analysis\Figs\fig3-4.png)

Note: Coefficient values measured on the x-axes represent the marginal changes in logged odds of higher ACA favorability (on a seven-point scale) given a 1-unit increase of the specified predictor. The opinion and presidential approval control variables are also continuous seven-point scales. The partisanship variables are all dummies, with independents excluded as the baseline categories. No other variables are included in the model. The constants for the models are not shown. Sample sizes for the models were 124 for primed individual mandate model, 321 for the unprimed individual mandate model, 129 for the primed dependent coverage model, and 326 for the corresponding unprimed model. See Appendix A for an explanation of the disparities in group sample sizes. The pseudo R^2^'s for the models range from 0.17 to 0.36. Full models are shown in tabular form in the supplementary online materials, Appendix B, tables B-2 and B-3.

The models show that opinions of specific provisions significantly affect overall opinions of the ACA in all treatment groups. As support for either of the two provisions increases, support for the ACA overall also increases. However, in the individual mandate models, the effect of provision-specific opinions is stronger for the primed group than the unprimed group, with a difference in logged odds of about 0.31 (t = 2.06, p < 0.05) on the seven-point scale. What this means is that the prime made the effect of individual mandate opinions on ACA opinions stronger relative to the unprimed group.

The individual mandate results also reveal that priming this provision makes partisanship an insignificant predictor of overall ACA opinion. This change is statistically meaningful, based on the significant interaction term (t = -4.25, p < 0.001). Given the general strength of partisanship as a predictor of individual political attitudes, including those related to the ACA [@Henderson2011; @Jacobs2016], the fact that partisanship can be rendered an insignificant predictor of ACA opinion is striking. This suggests that opponents of the law may be able to overcome partisan divisions by appealing to design aspects such as the individual mandate. Chi^2^-tests of joint significance of the interactions also confirm that the predictors of primed and unprimed individual mandate groups are indeed different overall (Chi^2^ = 17.11, p < 0.01).

There is no consistent or significant difference in the primed and unprimed coefficients for the dependent coverage group models (Chi^2^ = 2.66, p = 0.62; no single interaction term is significantly different from zero). This may be a result of the low variance in opinions of the dependent coverage provision. Most respondents (almost 82 percent) liked this provision at least somewhat, and only eight percent had even a slightly unfavorable view of it.

An alternative specification of these models using OLS instead of order logit does not qualitatively change these results. As mentioned, the coefficients of interest are also not affected appreciably by the inclusion of other demographic variables. However, logit models with the dichotomous ACA approval variable used in the t-tests above as the dependent variable, the priming effects described in the individual mandate models disappear. This is because most of the changes being picked up by the ordered logit models are occuring in the lower range of the ACA approval scale. Priming the individual mandate affects those who already oppose the ACA, which means the dichotmous variable (favor or oppose) does not change. This presents an important caveat to the analysis of hypothesis 2: prime-driven changes in the predictors of opinion may only occur among those already opposed to the law.

As with hypothesis 1, reverse causality/ opposite causal ordering may threaten our ability to draw causal inferences. It may be the case that ACA opinions are also causing changes in provision-specific opinions, depending on which was asked about first. Models in which the provision opinions are the dependent variables and overall ACA opinions are controlled indicate that ACA opinions do indeed affect provision-specific opinions. However, there are no significant differences in the effects of ACA opinions based on priming. The model coefficients, tested jointly as above, are also not significantly different. The threat of endogeneity is still present, and future work may be needed to get unbiased estimates of the conclusions I make here. Still, there is tentative support for my hypotheses, and reverse causality does not appear to directly explain those results.

## Discussion

This study has explored the nature of public opinion on the ACA, specifically investigating how two provisions in the law affect overall opinions. One policy, the dependent coverage provision, was found to be a powerful slam dunk for supporters, significantly increasing general ACA support relative to the unprimed group. When respondents answered questions about this provision first, their opinions of the ACA were more favorable than those of the unprimed group. While priming the individual mandate, a potential wedge issue, did not affect opinions overall, it did for the subset who opposed the individual mandate. Furthermore, priming this more controversial provision changed the way people formed their opinions of the ACA. The individual mandate prime made respondents' opinions of the provision even more salient and their partisanship less salient in expressing their favorability toward the law as a whole. Both the slam dunk issue and the wedge issue worked as predicted.

What does this mean for the ACA's political future? I will not be able to predict precise changes in public opinion, but I can comment on general trends to watch. First, many of the ACA's more popular slam dunk provisions are safe. Despite the promises of Republican presidential candidates and others to "repeal every word of Obamacare," to take the wording of Republican Senator and presidential candidate Ted Cruz, portions like the dependent coverage provision are politically untouchable. There are probably also many parts of the law that have remained out of the public eye that will remain similarly safe (see chapter 2 for a discussion of the group-based politics that will predominate in these cases).

However, these slam dunks do not appear to have neutralized the potency of potential wedge issues in the ACA, something which supporters hoped to have already acomplished at this point in the law's implementation. Polls on the ACA have shown largley stable levels of support and opposition throughout the past six years. Now that implementation of the law has proceeded to implementation of the individual mandate and other significant provisions, the flashy slam dunks are much less relevant, having been buried in a deluge of rhetoric about choice and government overreach vs. responsibility and the public good.

As controversial features of the ACA like the individual mandate, Medicaid expansion, the employer mandate, and the Cadillac tax (in 2018) are implemented, their divisiveness will become more apparent, and there is little that can be done to prevent it at this point. My survey experiment shows that interpretive feedback effects are strong for the individual mandate, enough to significantly increase the level of opposition to the ACA overall. Keep in mind that I did not even try to present the mandate to respondents in an unfavorable light. My questions did not mention the mandate's more detestable features, like the fine for noncompliance. Health reform opponents will certainly be less kind in talking about this provision. While I did not directly test how people would behave if primed with both the individual mandate and the dependent coverage provision, the fact that the wedge still works as opponents would intend at this point is not good news for reformers. Parts of the ACA are still very much ground zero for political conflict, and will be for the foreseeable future.

What we will see in the coming years is the political equivalent of trench warfare over the ACA, with both sides fighting fiercely over inches they will probably not hold for long, barring some major external shock such as a wave election. While the larger status quo prevails, the wedges will continue to be driven in, casting the ACA in terms that are certain to divide the public and reinforce the conflict. With no slam dunks left at their disposal to advance positive interpretations of the law, ACA supporters must now hope the law can hold together long enough for more people to benefit and for opponents to give up the fight as they try to hold together what support they do have on their side of the wedge issues. While reformers did make a swift and powerful initial advance in passing the ACA and implementing its easier provisions, the opposition has now had time to firmly entrench itself and hold its position, forcing ACA supporters to do the same. Opponents, however, have no countervailing slam dunks that would effect universal opposition to the ACA, so a quick, decisive victory was never a realistic option for them, and will not be for either side in the foreseeable future.

Speaking more generally on the implications of my study for policy feedback theory, the data presented support feedback scholars' assumption that policies can affect the mass public's opinions through the way they interpret the law. This is entirely separate from the resource-based effects that are observed for beenficiaries of a program. More research could be performed on what exactly constitutes a successful slam dunk. Do these need to be less severable from the more controversial parts of a proposal in order to succeed? How have they been used effectively (or ineffectively) in the past? Can any of these concepts be measured reliably? Such studies would build on my results and advance our understanding of mass feedback effects even further.

## References

[^1]: @Jacobs2016 provides good insights into the effects tangible benefits have had on the opinions of those who have received them.

[^2]: In fact, the dependent coverage provision probably works against a major purpose of the individual mandate: to bring healthy (often young!) people into the newly redesigned individual insurance market. Keeping young people in the group insurance market (on their parents' plans) by fiat will keep most of them from even considering entering the individual market, especially since the dependent coverage provision was implemented years before the new individual marketplaces, raising the likelihood of adverse selection in those markets at least marginally. Still, the political benefits of including the dependent coverage provision as a slam dunk were expected to outweigh the policy costs. It would have done no good to have designed an individual market that had the best chance of long-term actuarial success if political opposition had led to the ACA's repeal before implementation.

[^3]: The lower compensation is a result of not having the other two experiments in this wave, resulting in a much shorter questionnaire.

[^4]: The coefficients in the final models all change by less than 0.01 when the other controls are included, with no changes in significance. Measures of model fit barely change at all. I am confident that the more parsimonious model reported in the text and Figure 4 explains as much of the relevant variation as I can with the measures I collected. This, of course, does not preclude unobserved factors from having effects on ACA opinion. However, I am most interested in the changes in coefficients, which are extremely robust across specifications of the model and appear to be driven by random assignment to treatment group, strengthening our ability to draw causal inferences. There are also no significant changes in the models when presidential approval is omitted. I include presidential approval because opinions of the ACA are strongly correlated with opinions of President Obama (as the final models show).

